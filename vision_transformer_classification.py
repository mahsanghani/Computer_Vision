# -*- coding: utf-8 -*-
"""vision-transformer-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16vZnKahYohnYB0Zh87xPwsY2AGJrPEP8

## Training the Vision Transformer on a Custom Dataset

In this notebook, we are going to fine-tune a pre-trained Vision Transformer (which can be found from [Huggingface](https://github.com/huggingface/transformers)) on a Custom Dataset. For this notebook we will be using the Rock, Paper, Scissors dataset which can be found [here](https://public.roboflow.com/classification/rock-paper-scissors/1). This dataset is a collection of 2925 images images in 3 different classes. This tutorial is based on Huggingface's [Fine tuning the Vision Transformer on CIFAR 10 notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb).

### Accompanying Blog Post

We recommend that you follow along in this notebook while reading the blog post on [How to Train the HuggingFace Vision Transformer On a Custom Dataset](blog.roboflow.com/how-to-train-the-huggingface-vision-transformer-on-a-custom-dataset/) concurrently.


We will prepare the data using [Roboflow's Preprocessing Tools](https://docs.roboflow.com/image-transformations/image-preprocessing), and train the model using this notebook. 

### Steps Covered in this Tutorial

In this tutorial, we will walk through the steps required to train a Vision Transformer on your custom classification data.

To train our image classifier we take the following steps:

* Install Vision Transformer dependencies
* Download custom Image Classification data using Roboflow
* Use the Vision Transformer Feature Extractor
* Run the Vision Transformer training procedure
* Evaluate the Vision Transformer on a test image
* Export the Vision Transformer model for future inference


### **About**

[Roboflow](https://roboflow.com) enables teams to deploy custom computer vision models quickly and accurately. Convert data from to annotation format, assess dataset health, preprocess, augment, and more. It's free for your first 1000 source images.

**Looking for a vision model available via API without hassle? Try Roboflow Train.**

![Roboflow Wordmark](https://i.imgur.com/dcLNMhV.png)

Let's start by installing the relevant libraries.
"""

!pip install -q git+https://github.com/huggingface/transformers

"""# Download the Data 

We'll preprocess and download our dataset from Roboflow. To preprocess the images, change the size of the image to 224x224. To download the dataset, use the "**Folder Structure**" export format.

To get your data into Roboflow, follow the [Getting Started Guide](https://blog.roboflow.ai/getting-started-with-roboflow/).

Note: This data has already been preprocessed through Roboflow; we HIGHLY reccommend you follow the [accompanying blog](blog.roboflow.com/how-to-train-the-huggingface-vision-transformer-on-a-custom-dataset/) as you go through this notebook.

![folder.PNG](data:image/png;base64,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)
"""

!curl -L "https://public.roboflow.com/ds/jJ169fVCbx?key=PCvoSUc3pX" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip

"""Next, convert the folder structure dataset into a PyTorch dataset format using PyTorch's ImageFolder dataset structure:"""

import torchvision
from torchvision.transforms import ToTensor

train_ds = torchvision.datasets.ImageFolder('/content/train/', transform=ToTensor())
valid_ds = torchvision.datasets.ImageFolder('/content/valid/', transform=ToTensor())
test_ds = torchvision.datasets.ImageFolder('/content/test/', transform=ToTensor())

"""## Define the Model

Here we define the model.

The model itself uses a linear layer on top of a pre-trained `ViTModel`. We place a linear layer on top of the last hidden state of the [CLS] token, which serves as a good representation of an entire image. We also add dropout for regularization.

**Note:** The Vision Transformer pretrained model can be used as a regular PyTorch layer.
"""

from transformers import ViTModel
from transformers.modeling_outputs import SequenceClassifierOutput
import torch.nn as nn
import torch.nn.functional as F

class ViTForImageClassification(nn.Module):
    def __init__(self, num_labels=3):
        super(ViTForImageClassification, self).__init__()
        self.vit = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)
        self.num_labels = num_labels

    def forward(self, pixel_values, labels):
        outputs = self.vit(pixel_values=pixel_values)
        output = self.dropout(outputs.last_hidden_state[:,0])
        logits = self.classifier(output)

        loss = None
        if labels is not None:
          loss_fct = nn.CrossEntropyLoss()
          loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
        if loss is not None:
          return logits, loss.item()
        else:
          return logits, None

"""## Define the Model Parameters

To train this model, we will train in 3 epochs, with a batch size of 10 and a learning rate of 2e-5:
"""

EPOCHS = 3
BATCH_SIZE = 10
LEARNING_RATE = 2e-5

"""We will use the pretrained Vision Transformer feature extractor, an Adam Optimizer, and a Cross Entropy Loss function."""

from transformers import ViTFeatureExtractor
import torch.nn as nn
import torch
# Define Model
model = ViTForImageClassification(len(train_ds.classes))    
# Feature Extractor
feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-224-in21k')
# Adam Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
# Cross Entropy Loss
loss_func = nn.CrossEntropyLoss()
# Use GPU if available  
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') 
if torch.cuda.is_available():
    model.cuda()

"""## Train the Model"""

import torch.utils.data as data
from torch.autograd import Variable
import numpy as np

print("Number of train samples: ", len(train_ds))
print("Number of test samples: ", len(test_ds))
print("Detected Classes are: ", train_ds.class_to_idx) 

train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4)
test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4) 

# Train the model
for epoch in range(EPOCHS):        
  for step, (x, y) in enumerate(train_loader):
    # Change input array into list with each batch being one element
    x = np.split(np.squeeze(np.array(x)), BATCH_SIZE)
    # Remove unecessary dimension
    for index, array in enumerate(x):
      x[index] = np.squeeze(array)
    # Apply feature extractor, stack back into 1 tensor and then convert to tensor
    x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))
    # Send to GPU if available
    x, y  = x.to(device), y.to(device)
    b_x = Variable(x)   # batch x (image)
    b_y = Variable(y)   # batch y (target)
    # Feed through model
    output, loss = model(b_x, None)
    # Calculate loss
    if loss is None: 
      loss = loss_func(output, b_y)   
      optimizer.zero_grad()           
      loss.backward()                 
      optimizer.step()

    if step % 50 == 0:
      # Get the next batch for testing purposes
      test = next(iter(test_loader))
      test_x = test[0]
      # Reshape and get feature matrices as needed
      test_x = np.split(np.squeeze(np.array(test_x)), BATCH_SIZE)
      for index, array in enumerate(test_x):
        test_x[index] = np.squeeze(array)
      test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))
      # Send to appropirate computing device
      test_x = test_x.to(device)
      test_y = test[1].to(device)
      # Get output (+ respective class) and compare to target
      test_output, loss = model(test_x, test_y)
      test_output = test_output.argmax(1)
      # Calculate Accuracy
      accuracy = (test_output == test_y).sum().item() / BATCH_SIZE
      print('Epoch: ', epoch, '| train loss: %.4f' % loss, '| test accuracy: %.2f' % accuracy)

"""## Evaluate on a Test Image

Finally, let's evaluate the model on a test image:
"""

import matplotlib.pyplot as plt
import numpy as np

EVAL_BATCH = 1
eval_loader  = data.DataLoader(valid_ds, batch_size=EVAL_BATCH, shuffle=True, num_workers=4) 
# Disable grad
with torch.no_grad():
    
  inputs, target = next(iter(eval_loader))
  # Reshape and get feature matrices as needed
  print(inputs.shape)
  inputs = inputs[0].permute(1, 2, 0)
  # Save original Input
  originalInput = inputs
  for index, array in enumerate(inputs):
    inputs[index] = np.squeeze(array)
  inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))

  # Send to appropriate computing device
  inputs = inputs.to(device)
  target = target.to(device)
 
  # Generate prediction
  prediction, loss = model(inputs, target)
    
  # Predicted class value using argmax
  predicted_class = np.argmax(prediction.cpu())
  value_predicted = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(predicted_class)]
  value_target = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(target)]
        
  # Show result
  plt.imshow(originalInput)
  plt.xlim(224,0)
  plt.ylim(224,0)
  plt.title(f'Prediction: {value_predicted} - Actual target: {value_target}')
  plt.show()

"""## Save the Entire Model

We can save the entire model as follows:
"""

torch.save(model, '/content/model.pt')

"""## Export Trained Model

Now that you have trained your custom vision transformer, you can export the trained model you have made here for inference on your device elsewhere
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')

# %cp /content/model.pt /content/gdrive/My\ Drive

"""## Use your Exported Model"""

MODEL_PATH = '/content/model.pt'
model = torch.load(MODEL_PATH)
model.eval()